{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear equation\n",
    "\n",
    "\n",
    "Consider the following two-period consumption-saving problem:\n",
    "\n",
    "\\begin{equation}\n",
    "\\max_{c_1,c_2,s_1} u(c_1) + \\beta \\mathbb{E}_1[u(c_2)]\n",
    "\\end{equation}\n",
    "\n",
    "subject to \n",
    "\\begin{align}\n",
    "c_1 +s_1 &= Y_1 \\\\ \n",
    "c_2 &= Y_2 +(1+R_2)s_1.\n",
    "\\end{align}\n",
    "\n",
    "Here, $R_2$ is assumed to be known in period 1 while $Y_2$ is a random variable that realizes in period 2. \n",
    "\n",
    "We want to calculate a solution to this problem. Recall the Euler equation is given by:\n",
    "\n",
    "$$ u'(Y_1 - s_1) = \\beta \\mathbb{E}_1[ u'( (1+R_2)s_1+Y_2)](1+R_2). $$ \n",
    "\n",
    "Here I have substituted out consumptions, $c_1$ and $c_2$ using the budget constraints. Under a set of standard assumptions on $u$, the left hand side is a strictly increasing function of $s_1$ and the right hand side is a strictly decreasing function of it. Hence, there is at most one value of $s_1$ that satisfies the Euler equation. Moreover, if Inada condition is satisfied, one can show the existence of such $s_1$.  \n",
    "\n",
    "Suppose we have specified all the parameters in this model, i.e. the utility function $u$, the period-1 income $Y_1$, the real interest rate $R_2$, and a distribution of $Y_2$. To obtain the optimal savings, $s_1$, we need to solve the Euler equation, but in general it does not permit an analytical solution. Hence we need to solve the equation _numerically_.\n",
    "\n",
    "Solving the Euler equation amounts to solving a _root finding problem_:\n",
    "\n",
    "$$ 0 = f(x), $$\n",
    "\n",
    "where $f:\\mathbb{R} \\rightarrow \\mathbb{R}$ is defined by $f(x) := \\beta \\mathbb{E}_1[ u'( (1+R_2)x+Y_2)](1+R_2)- u'(Y_1 - x)$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If we consider a static, endowment economy with competitive market, each individual $i \\in \\{1,2,...,I\\}$ solves\n",
    "\\begin{equation}\n",
    "\\max_{ \\{c_m^i\\}_{m=1}^M } u(c_1^i,c_2^i,...,c_m^i)\n",
    "\\end{equation}\n",
    "subject to\n",
    "\\begin{equation}\n",
    "\\sum_{m=1}^M p_m c_m^i \\le \\sum_{m=1}^M p_m e_m^i.\n",
    "\\end{equation}\n",
    "Then, letting $x^i_m(\\mathbf{p})$ be the Marshallian demand function for individual $i$ and for good $m$, an equilibrium price vector $\\mathbf{p}=(p_1,p_2,...,p_M)\\in \\mathbb{R}_+^M$ satisfies, for $m=1,...,M$,\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^I x^i_m(\\mathbf{p}) - \\sum_{i=1}^I e^i_m =0.\n",
    "\\end{equation}\n",
    "Solving for an equilibrium price vector amounts to solving a system of $M$ nonlinear equations. (Actually we have $M-1$ unknowns and $M-1$ equations, because the demand function is homogeneous of degree 0 and because Walras' law holds.) I.e. we need to solve $0=f(x)$, this time $f:\\mathbb{R}^{M-1} \\rightarrow \\mathbb{R}^{M-1}$. This is again a root finding problem, while it is a multi-dimensional one. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sum, we encounter many situations in economics in which we need to solve root finding problems. In today's lecture we limit our attention to a single dimension case and study some algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What we study below are iterative methods that (try to) compute a solution to $f(x)=0$ for a continuous $f:\\mathbb{R} \\rightarrow \\mathbb{R}$. These methods generate a sequence $(x_0,x_1,...,x_n, x_{n+1},...)$ until a convergence criterion is satisfied. \n",
    "\n",
    "We care about the following:\n",
    "- Rate of convergence. When converges, how fast does it converge?\n",
    "- Robustness/reliability. Is it guaranteed that a method find an approximate solution?\n",
    "\n",
    "### Rate (speed) of convergence\n",
    "Consider a sequence $\\{x_n\\}_{n=0}^\\infty$ such that $x_n \\rightarrow x$. We say that the rate of convergence is linear if\n",
    "$$ \\lim_n \\frac{|x_{n+1} - x |}{|x_n -x|} \\le C $$\n",
    "for some constant $C>0$. (Example. $x_n = 1 + \\left( \\frac{1}{2} \\right)^n$)\n",
    "\n",
    "Similarly, the rate of convergence is said to be quadratic if\n",
    "$$ \\lim_n \\frac{|x_{n+1} - x |}{|x_n -x|^2} \\le C $$\n",
    "for some constant $C>0$. (Example. $x_n = 1 + \\left( \\frac{1}{n} \\right)^{2^n}$)\n",
    "\n",
    "Convergence rate is said to be _superlinear_ if\n",
    "$$ \\lim_n \\frac{|x_{n+1} - x |}{|x_n -x|^2} = 0. $$\n",
    "(Example. $x_n = 1 + \\left( \\frac{1}{n} \\right)^{n}$)\n",
    "\n",
    "Similarly, the rate of convergence is said to be sublinear if\n",
    "$$ \\lim_n \\frac{|x_{n+1} - x |}{|x_n -x|^2} = 1. $$ \n",
    "\n",
    "The rate of convergence is said to be of order $\\alpha \\ge 1$ if\n",
    "$$ \\lim_n \\frac{|x_{n+1} - x |}{|x_n -x|^\\alpha} \\le C $$\n",
    "for some constant $C>0$.\n",
    "\n",
    "Note that in most cases convergence of an algorithm is not guaranteed, and therefore the rate of convergence we will see is oftentimes conditional on the algorithm converging.\n",
    "\n",
    "\n",
    "### Robustness/reliability\n",
    "Requiring that a method _always_ converge may be too much. For most methods, whether they converge or not depend crucially on the initial condition (in particular whether a starting value is sufficiently close to a solution). Among the methods we study, Newton and quasi-Newton methods are not guaranteed to converge, but SciPy subroutines somewhat robustify these methods using _line search_ algorithms. Another example of robustification is to re-start a method using a randomized initial condition when the method fails to find a solution. (I haven't figured out whether SciPy root-finding routines use randomization. For example, Christopher Sims's csolve function <http://sims.princeton.edu/yftp/optimize/>, though written in R and Matlab, uses random search direction.)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bisection method\n",
    "\n",
    "There are $a, b \\in \\mathbb{R}$ such that\n",
    "\n",
    "- $f$ is a continuous function on $[a,b]$, and that\n",
    "- $f(a)f(b)<0$.\n",
    "\n",
    "The second condition means that the function $f$ takes different signs at two boundary points of the interval $[a,b]$, i.e. either $f(a)<0<f(b)$ or $f(a)>0>f(b)$. Then it follows from the intermediate value theorem that there is $x \\in (a,b)$ such that $f(x)=0$.\n",
    "\n",
    "\n",
    "\n",
    "The algorithm is as follows (taken from Judd, p148):\n",
    "0. Initialize $L$ and $U$ such that $L<U$ and  $f(L)\\times f(U)<0$. Also fix stopping criterion parameters, $\\epsilon$ and $\\delta$. \n",
    "1. Compute a midpoint $M=(L+U)/2$ and calculate $f(M)$.\n",
    "2. If $f(M)f(L)<0$, set $U=M$ but do not change $L$. Otherwise, set $L=M$ and leave $U$ unchanged.\n",
    "3. Convergence judgement: if $U-L \\le \\epsilon(1+|L|+|U|)$ or if $|f(M)|\\le \\delta$ then STOP and report solution at $M$; otherwise go to STEP 1.\n",
    "\n",
    "Advantages: Convergence is guaranteed (i.e. the method is robust); $f$ does not have to be differentiable.  \n",
    "\n",
    "Disadvantages: Convergence rate is (sort of) linear; Cannot be generalized to multi-dimensional cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's define a function that implements this algorithm. The function \n",
    "- Takes $f$, $L$, $M$, $\\epsilon$, $\\delta$ as input, and\n",
    "- Returns $M$ as output.\n",
    "\n",
    "We may want to use certain numbers as default values for $\\epsilon$ and $\\delta$ unless a user specifies these numbers explicitly. In other words, we may want to make $\\epsilon$ and $\\delta$ as _optional input_. In Python this can be done using **keyword arguments**.\n",
    "\n",
    "We can pass a function as input for a function easily in Python.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisection(f,a,b, xtol = 1e-6, ftol = 1e-6, print_int = True):\n",
    "    \n",
    "    # This function implements the bisection method to find a root of f between a and b.\n",
    "    # Required: a<b, f is continuous on [a,b], and f(a)f(b)<0.\n",
    "    \n",
    "    \n",
    "    L,U = a,b\n",
    "    fL, fU = f(L), f(U)\n",
    "    \n",
    "    i_iter = 0\n",
    "    max_iter = 100\n",
    "    \n",
    "    while i_iter<=max_iter:\n",
    "        i_iter += 1\n",
    "        M = 0.5*(L+U)\n",
    "        fM = f(M)\n",
    "\n",
    "        if fM*fL<0:\n",
    "            U, fU = M, fM\n",
    "        else:\n",
    "            L, fL = M, fM\n",
    "        \n",
    "        if print_int:\n",
    "            print('Iteration step = ' + str(i_iter) + ', midpoint = ' + str(M))\n",
    "            \n",
    "        if U-L <xtol*(1.0+abs(L)+abs(U)) or abs(fM)<ftol:\n",
    "            print('Converged.')\n",
    "            break\n",
    "\n",
    "    if i_iter == max_iter:\n",
    "        print('Max. number of iteration reached. A solution may not be accurate.')\n",
    "            \n",
    "    return M   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate this function using some test functions:\n",
    "\n",
    "1. $test(x) = x^2 -x -1$, $L=0.0$, and $U=2.0$;\n",
    "2. $test1(x) = \\sin(4(x - 0.25)) + x + x^{20} - 1$, $L=0.0$, and $U=1.0$.\n",
    "3. $test2(x) = 2\\ln(x)+0.3$, $L=0.5$, and $U=\\pi$.\n",
    "\n",
    "These functions are simple in that they can be defined within a single line. Although we can use the def statement to define these functions, we can use the _lambda_ statement to define them as _anonymous functions_. This functionality is convenient if we want to define some simple functions on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = lambda x: x**2-x-1\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "test2 = lambda x: np.sin(4 * (x - 0.25)) + x + x**20 - 1.0\n",
    "\n",
    "test3 = lambda x: 2*np.log(x)+0.3\n",
    "\n",
    "xsol = bisection(test, 0.0,2.0)\n",
    "print(xsol)\n",
    "print()\n",
    "\n",
    "xsol2 = bisection(test2, 0.0, 1.0)\n",
    "print(xsol2)\n",
    "print()\n",
    "\n",
    "xsol3 = bisection(test3, 0.5, np.pi)\n",
    "print(xsol3)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SciPy, scipy.optimize.bisect implements hte bisection method. <https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.bisect.html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton-Raphson method (or Newton's method)\n",
    "\n",
    "Newton-Raphson method, or simply Newton's method, is a derivative-based method to search for a root of a function. Although the method is generalizable to multi-dimensional cases, here we focus on a single dimensional case.\n",
    "\n",
    "If $f$ is $C^1$, then the first order Taylor approximation gives us:\n",
    "\n",
    "$$ f(x) = f(a) + f'(a)(x-a) + o(|x-a|).$$\n",
    "\n",
    "The little-o notation here means a function of $x$ such that $o(|x-a|)/|x-a| \\rightarrow 0$ as $x \\rightarrow a$.\n",
    "\n",
    "NR method updates its estimate of a root by \n",
    "\n",
    "$$ x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}.$$\n",
    "\n",
    "This is equivalent to \n",
    "\n",
    "$$ 0 = f(x_n) +f'(x_n)(x_{n+1} - x_n),$$\n",
    "\n",
    "which approximately solves $f(x_{n+1}) = 0$ ($a$ is set to $x_n$). \n",
    "\n",
    "The algorithm is as follows (taken from Judd, p.152):\n",
    "0. Initialize $x_0$ and $n=0$. Also fix stopping criterion parameters, $\\epsilon$ and $\\delta$. \n",
    "1. Compute a next iterate: $x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$.\n",
    "2. Convergence judgement: if $|x_{n+1}-x_n|\\le \\epsilon (1+|x_n|)$, go to step 3. Otherwise, go to step 1.\n",
    "3. If $|f(x_{n+1}|\\le \\delta$, report success in finding a zero; otherwise, report failure.\n",
    "\n",
    "Advantages: Convergence is quadratic (required: $x_0$ has to be \"sufficiently close\" to a solution). \n",
    "\n",
    "Disadvantages: Convergence is not guaranteed (divergence, cycles, the limited domain problem, etc.). Derivative information is needed. \n",
    "\n",
    "A general lesson here is that there is a trade-off between reliability and speed. \n",
    "\n",
    "We will discuss shortly _quasi-Newton_ methods that bypass the derivative evaluation problem and line-search methods that can relax the non-convergence problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def NRmethod(f,g,x0, xtol=1e-6, ftol=1e-6, max_iter=1000):\n",
    "    \n",
    "    x, fx, gx = x0, f(x0), g(x0)\n",
    "    \n",
    "    i_iter = 0\n",
    "    \n",
    "    while i_iter<=max_iter:\n",
    "        i_iter += 1\n",
    "        \n",
    "        dx = - fx/gx\n",
    "        xp = x +dx\n",
    "        \n",
    "        print('Iteration step n = ' + str(i_iter) + ', x[n+1] = ' + str(xp))\n",
    "        \n",
    "#        if abs(dx) <= xtol*(1+abs(x)):\n",
    "        if abs(dx) <= xtol:\n",
    "            x, fx = xp, f(xp)        \n",
    "            break\n",
    "            \n",
    "        x, fx, gx = xp, f(xp), g(xp)        \n",
    "\n",
    "\n",
    "    if i_iter == max_iter:\n",
    "        print('Max. number of iteration reached. A solution may not be accurate.')\n",
    "        return x\n",
    "    elif abs(fx)<=ftol: \n",
    "        print('Newton-Raphson method converged.')\n",
    "        return x\n",
    "    else:\n",
    "        print('Newton-Raphson method failed to find a solution.')\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative of the test function\n",
    "\n",
    "dtest = lambda x: 2*x-1.0\n",
    "\n",
    "x_init = 1.0\n",
    "xsol_NR = NRmethod(test,dtest, x_init)\n",
    "print(xsol_NR)\n",
    "\n",
    "\n",
    "alpha = 1e-8\n",
    "test_scaled = lambda x: test(x/alpha)\n",
    "dtest_scaled = lambda x: dtest(x/alpha)/alpha\n",
    "\n",
    "xsol_NR_scaled = NRmethod(test_scaled,dtest_scaled, x_init*alpha)\n",
    "print(xsol_NR_scaled/alpha)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Why do we have abs(dx) <= xtol*(1+abs(x)), not abs(dx) <= xtol? Change that part and examine how the scaling can result in a different convergence speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following I use the function NRmethod2, which is built on the above NRmethod, to return sequences of $x_n$ and $f(x_n)$ as well as a solution when an optional argument _seq_out_ is set to True. This extension will be a part of the biweekly assignment, so I do not share the code at this point. I saved NRmethod2 function in the .py file named NonLinSolve.py to call from this notebook. So obviously you cannot run the following cell now. \n",
    "\n",
    "The following cell provides an example of vectorization and plot. \n",
    "\n",
    "For a more detailed introduction of matplotlib, see e.g. <https://lectures.quantecon.org/py/matplotlib.html>.\n",
    "It is also useful to look at sample gallery <http://matplotlib.org/gallery.html>. If you click a figure, you will jump to a page with a python script that creates it, so that you can learn how to create them!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import NonLinSolve as NR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x_grid = np.linspace(1.58, 2.05, 100)\n",
    "test_vec = np.vectorize(test)\n",
    "fx_grid = test_vec(x_grid)\n",
    "\n",
    "xsol_NR2, x_seq, fx_seq = NR.NRmethod2(test,dtest, 2.0, seq_out = True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x_grid, fx_grid, 'b-', linewidth=1, label='Test function')\n",
    "ax.plot(x_grid, np.zeros_like(fx_grid), '--', linewidth=1, label='Horizontal axis')\n",
    "ax.plot(x_seq, fx_seq, 'r.', label='Newton iterates')\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_yticks([-1.0, 0, 1.0, 2.0])\n",
    "ax.set_title('Test plot')\n",
    "\n",
    "for n in range(len(x_seq)):\n",
    "    ax.annotate(str(n), (x_seq[n],fx_seq[n]+0.05))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secant method\n",
    "\n",
    "Computing a derivave can be costly. If you do not have an analytical expression for $f'$, then you need to do a finite difference approximation, which requires function evaluation twice.  \n",
    "\n",
    "The secant method uses the function value calculated in a previous step to approximately obtain a slope of $f$, i.e. it uses \n",
    "\n",
    "$$ \\frac{f(x_n) -f(x_{n-1})}{x_n - x_{n-1}}$$\n",
    "\n",
    "in place of $f'$. Hence,\n",
    "\n",
    "$$ x_{n+1} = x_n - \\frac{f(x_n)(x_n - x_{n-1})}{f(x_n) -f(x_{n-1})}.$$\n",
    "\n",
    "The rate of convergence is less than 2, but faster than linear.\n",
    "\n",
    "As you can see, we need $x_n$ and $x_{n-1}$ to calculate $x_{n+1}$. \n",
    "\n",
    "Let's first implement this algorithm that takes $f$,  $x_0$, and $x_1$ as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def secant1(f,x0,x1, xtol=1e-6, ftol=1e-6, max_iter = 1000):\n",
    "    \n",
    "    x_now, x_prev, f_now, f_prev = x1, x0, f(x1), f(x0)\n",
    "\n",
    "    \n",
    "    i_iter = 0\n",
    "    \n",
    "    while i_iter<=max_iter:\n",
    "        i_iter += 1\n",
    "        \n",
    "        dx = - f_now*(x_now-x_prev)/(f_now-f_prev)\n",
    "        x_next = x_now + dx\n",
    "        \n",
    "        x_now, x_prev, f_now, f_prev = x_next, x_now, f(x_next), f_now        \n",
    "        \n",
    "        print('Iteration step n = ' + str(i_iter) + ', x[n+1] = ' + str(x_now))\n",
    "        \n",
    "        if abs(dx) <= xtol*(1+abs(x_now)):\n",
    "            break\n",
    "\n",
    "    if i_iter == max_iter:\n",
    "        print('Max. number of iteration reached. A solution may not be accurate.')\n",
    "        return x_now\n",
    "    elif abs(f_now)<=ftol: \n",
    "        print('Secant method converged.')\n",
    "        return x_now\n",
    "    else:\n",
    "        print('Secant method failed to find a solution.')\n",
    "        return x_now    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsol_secant =  secant1(test,2.0,1.0, xtol=1e-6, ftol=1e-6)\n",
    "\n",
    "print(xsol_secant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SciPy, scipy.optimize.newton() uses the Newton-Raphson method if the derivative function is supplied and the secant method otherwise. From the documentation it is not clear how $x_1$ is obtained when the secant method is used (only $x_0$ is supplied by the user), but my guess is that in the very first step of the iteration the function is evaluated in a neighborhood of $x_0$ and then the algorithm starts from there.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brent's Method (or Brent-Dekker Method)\n",
    "This method combine Bisection and Secant methods. In reality, I use this method most often in my computations, as the method balaces its speed and robustness. In SciPy, you can call Brent's method by scipy.optimize.brentq(). <https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.brentq.html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed point\n",
    "\n",
    "For any set $S$ and $f:S\\rightarrow S$, we say $s\\in S$ is a fixed point of $f$ if and only if\n",
    "$$ s = f(s).$$\n",
    "(A fixed poing of a correspondence is defined in the same way.)\n",
    "\n",
    "When $S \\subset \\mathbb{R}$, finding a fixed point of $f$ is equivalent to finding a root of $\\tilde{f}(s):=f(s)-s$. Hence we may use above methods to obtain a root of $\\tilde{f}$. \n",
    "\n",
    "However, we may alternatively exploit the fixed point expression by generateing iterates:\n",
    "$$x_{n+1} = f(x_n), \\ \\mbox{for n=0,1,2,...}$$\n",
    "starting from some initial $x_0$. This method is called _fixed-point iteration_. Clearly, this only requires function evaluations.\n",
    "\n",
    "So, we may be able to find a root of $\\tilde{f}(x)=0$ by transforming it into a fixed point problem $f(x)=x$ and by fixed-point iteration.\n",
    "\n",
    "In the week 1 assignment, we considered\n",
    "$$  x_{n+1} = 1+\\frac{1}{x_n} $$ \n",
    "with $x_0>0$. This is indeed a fixed-point iteration for $f(x) := 1+1/x$. Its fixed point satisfies\n",
    "$$ x = 1 + 1/x,$$\n",
    "i.e.\n",
    "$$ x^2 -x-1 = 0.$$\n",
    "What we did was, effectively, to compute a positive root of this quadratic equation. You can check you answer with the formula $\\frac{1 + \\sqrt{5}}{2}$.\n",
    "\n",
    "In general, convergence is not guaranteed. An important case in which convergence is guaranteed is when $f$ is a _differentiable contraction mapping_. \n",
    "\n",
    "A differentiable contraction mapping on a convex compact set $D \\subset \\mathbb{R}^N$ is a $C^1$ function $f$ such that \n",
    "1. $f(D) \\subset D$, and\n",
    "2. $\\max_{x\\in D} ||J(x)|| <1$. (The norm of the Jacobian matrix is bounded by 1.)\n",
    "\n",
    "If $f$ is a differentiable contraction mapping on $D$, then it has a unique fixed point in $D$ and a successive iteration converges to the fixed point. The rate of convergence is linear. \n",
    "\n",
    "Going back to the Week assignment, one can show that, for sufficiently small $\\epsilon>0$, $f(x) := 1+1/x$ is a differentiable contraction mapping on $[1+\\epsilon, 1+1/(1+\\epsilon)]$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hands-on exercise** \n",
    "\n",
    "Consider \n",
    "$$ x^3-x-1 = 0.$$\n",
    "\n",
    "This implies both \n",
    "$$ x = (x+1)^{1/3}$$\n",
    "and\n",
    "$$ x = x^3 -1.$$\n",
    "\n",
    "Implement fixed-point iteration $x_{n+1} = f(x_n)$ from $x_0=1.0$, using\n",
    "1. $f(x) = (x+1)^{1/3}$ and\n",
    "2. $f(x) = x^3 -1.$\n",
    "\n",
    "To guard against non-convergence, you may set a max number of iteration to terminate the iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "\n",
    "1. Do the above hands-on exercise for the fixed-point iteration.\n",
    "\n",
    "2. Discuss the tradeoff between reliability and speed of nonlinear equation solving algorithms, using the bisection method and Newton-Raphson method as examples.\n",
    "\n",
    "3. Explain the difference between Dekker's method and Brent's method. <https://en.wikipedia.org/wiki/Brent%27s_method>\n",
    "\n",
    "\n",
    "4. You may need to find a value of $x$ such that the function $f$ has arguments other than $x$. (I.e. you need to solve $0=f(x,y,z,...)$ for given $(y,z,...)$.) The scipy routines such as scipy.optimize.bisect and scipy.optimize.netwon take care of such cases. Read <https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.bisect.html> and verbally explain how one can incorporate such a feature. \n",
    "\n",
    "5. [No need for submission] Familiarize yourself with the examples in <https://lectures.quantecon.org/py/matplotlib.html>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
